# This file was generated by Nuitka

# Stubs included by default
from __future__ import annotations
from base_service import BaseService
from llama_cpp import Llama
from typing import Any
from typing_extensions import Self
import hashlib
import json
import os
import re
import subprocess
import sys
import threading
import traceback

LLAMA_CPP_AVAILABLE = True
LLAMA_CPP_AVAILABLE = False
class AIAnalyzerService(BaseService):
    def __init__(self: Self, kernel: Any, service_id: str) -> None: ...
    def start(self: Self) -> Any: ...
    def stop(self: Self) -> Any: ...
    def request_analysis(self: Self, context_id: str) -> Any: ...
    def _run_analysis(self: Self, context_id: str) -> Any: ...
    def invalidate_suggestion_cache(self: Self, preset_name: str) -> Any: ...
    def _suggestion_publisher(self: Self, message: Any, level: Any, context: Any) -> Any: ...
    def _get_metrics_for_context(self: Self, target_context_id: str) -> list: ...
    def _summarize_metrics(self: Self, metrics: list) -> str: ...
    def _create_analysis_prompt(self: Self, summary_text: str) -> str: ...
    def _process_and_publish_ai_suggestions(self: Self, raw_data: Any, context_id: Any) -> Any: ...


__name__ = ...



# Modules used internally, to allow implicit dependencies to be seen:
import os
import json
import threading
import re
import hashlib
import traceback
import sys
import subprocess
import llama_cpp
import llama_cpp.Llama
import ntpath